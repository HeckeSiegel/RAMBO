{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import datetime\n",
    "import sys\n",
    "import sparkStructuredStreaming\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary to stream from Kafka topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.5 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"127.0.0.1:9092\" (local) //\"10.0.0.8:9092\" (BACC)\n",
    "bootstrap = \"127.0.0.1:9092\"\n",
    "hdfs_path = \"hdfs://0.0.0.0:19000\"\n",
    "output_dir = \"iex/quotes\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Spark Session + Kafkastream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# udf to convert epoch time to spark TimestampType\n",
    "get_timestamp = udf(lambda x : datetime.datetime.fromtimestamp(x/ 1000.0).strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "            .builder \\\n",
    "            .appName(\"KafkaIEXStructuredStreaming\") \\\n",
    "            .master(\"local[*]\") \\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = sparkStructuredStreaming.kafka_spark_stream(bootstrap)\n",
    "\n",
    "parsedDF = sss.stream_quotes(spark)       \n",
    "\n",
    "selectDF = parsedDF \\\n",
    "        .select(explode(array(\"quote_data\")))\\\n",
    "        .select(get_timestamp(\"col.latestUpdate\").cast(\"timestamp\").alias(\"timestamp\"),\"col.latestPrice\")\\\n",
    "        .withWatermark(\"timestamp\", \"15 hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some functions for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_chart(df,interval):\n",
    "    # use df with \"timestamp\", \"latestPrice\", \"Watermark\"\n",
    "    # get open, high, low prices for each time interval\n",
    "    interval_values = df.groupBy(\n",
    "        window(df.timestamp, interval))\\\n",
    "        .agg(max(\"latestPrice\").alias(\"high\"),\\\n",
    "            min(\"latestPrice\").alias(\"low\"),\\\n",
    "            min(\"timestamp\").alias(\"open_time\"))\\\n",
    "        .select(\"window.start\",\"window.end\",\"high\",\"low\",\"open_time\")\\\n",
    "        .withWatermark(\"start\", interval)\n",
    "    \n",
    "    # join to get opening price from opening time\n",
    "    chart = interval_values.join(df,interval_values.open_time == df.timestamp, \"left\")\\\n",
    "        .drop(\"open_time\",\"timestamp\")\\\n",
    "        .withColumnRenamed(\"latestPrice\",\"open\")\n",
    "        \n",
    "    return chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(spark, df, update, interval):\n",
    "    # simple moving average for the interval \"interval\"\n",
    "    \n",
    "    windowdf = df.select(window(df.timestamp, interval, update), df.latestPrice)\n",
    "    \n",
    "    windowdf.createOrReplaceTempView(\"windowdf_sql\")\n",
    "    \n",
    "    sma = spark.sql(\"\"\"SELECT windowdf_sql.window AS time, avg(windowdf_sql.latestPrice) AS average\n",
    "                    FROM windowdf_sql\n",
    "                    Group BY windowdf_sql.window\n",
    "                    \"\"\")   \n",
    "    return sma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import clear_output\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stream(df, epoch_id):\n",
    "    df = df.orderBy(\"timestamp\")\n",
    "    df_pd = df.toPandas()\n",
    "    clear_output(wait=True)\n",
    "    df_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average = moving_average(spark, selectDF, \"1 minutes\" ,\"8 minutes\")\n",
    "selectDF\\\n",
    "    .writeStream\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .trigger(processingTime = \"60 seconds\")\\\n",
    "    .foreachBatch(plot_stream)\\\n",
    "    .start()\\\n",
    "    .awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
