{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import datetime\n",
    "from pytz import timezone\n",
    "import sys\n",
    "from lib import sparkStructuredStreaming\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set-up to stream from Kafka topic + read and write from/to Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.5,org.elasticsearch:elasticsearch-spark-20_2.11:7.6.2 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"127.0.0.1:9092\" (local) //\"10.0.0.8:9092\" (BACC)\n",
    "bootstrap = \"127.0.0.1:9092\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use this for elasticsearch, otherwise it won't recognize date field\n",
    "get_datetime_kafka = udf(lambda x : datetime.datetime.fromtimestamp((x-7200000)/ 1000.0).strftime(\"%Y-%m-%d\"'T'\"%H:%M:%S\"))\n",
    "get_datetime_yahoo = udf(lambda x : ((timezone('Europe/Berlin').localize(x)).astimezone(timezone('UTC'))).strftime(\"%Y-%m-%d\"'T'\"%H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "            .builder \\\n",
    "            .appName(\"KafkaIEXStructuredStreaming\") \\\n",
    "            .master(\"local[*]\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "sqlContext = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Implement Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_strategy(pe_ratio):\n",
    "    if pe_ratio < 10:\n",
    "        action = \"buy\"\n",
    "    elif pe_ratio > 15:\n",
    "        action = \"sell\"\n",
    "    else:\n",
    "        action = \"hold\"\n",
    "        \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Backtesting with static Dataframes with Historical Data from Yahoo Finance (Pyspark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Read historical data from yahoo finance, write into Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "#from elasticsearch import Elasticsearch\n",
    "\n",
    "def history(symbol,period,interval):\n",
    "    #read historical data from yahoo finance into a pandas df\n",
    "    ticker = yf.Ticker(symbol)\n",
    "    pandas_history = ticker.history(period=period, interval=interval)\n",
    "    #tranform into spark df\n",
    "    pandas_history.reset_index(drop=False, inplace=True)\n",
    "    spark_history = sqlContext.createDataFrame(pandas_history)\n",
    "    #transform time, add symbol, add unique id\n",
    "    spark_history = spark_history.select(\"Open\",\"High\",\"Low\",\"Close\",\"Volume\",get_datetime_yahoo(\"Datetime\").cast(\"String\").alias(\"date\"))\n",
    "    spark_history = spark_history.withColumn(\"symbol\", lit(symbol))\n",
    "    spark_history = spark_history.withColumn('id',concat(col(\"date\"),col(\"symbol\")))\n",
    "    #write into elasticsearch\n",
    "    spark_history.write\\\n",
    "                .format(\"es\")\\\n",
    "                .mode(\"append\")\\\n",
    "                .option(\"es.resource\", interval+\"/history\")\\\n",
    "                .option(\"es.mapping.id\", \"id\")\\\n",
    "                .option(\"es.nodes\", \"127.0.0.1:9200\") \\\n",
    "                .save()\n",
    "    \n",
    "history(\"AAPL\",\"2d\",\"1m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Read historical data from Elasticsearch into Spark Dataframe and evaluate Strategy with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Trading Simulation / Performance Evaluation with realtime Streams (Spark Streaming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stream real time quotes from Kafka topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = sparkStructuredStreaming.kafka_spark_stream(bootstrap)\n",
    "\n",
    "parsedDF = sss.stream_quotes(spark)       \n",
    "\n",
    "selectDF_es = parsedDF \\\n",
    "        .select(explode(array(\"quote_data\")))\\\n",
    "        .select(\"col.*\",get_datetime_kafka(\"col.latestUpdate\").cast(\"String\").alias(\"date\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize results, either here with Plotly or write results into Elasticsearch -> Kibana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some functions for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_chart(df,interval):\n",
    "    # use df with \"timestamp\", \"latestPrice\", \"Watermark\"\n",
    "    # get open, high, low prices for each time interval\n",
    "    interval_values = df.groupBy(\n",
    "        window(df.timestamp, interval))\\\n",
    "        .agg(max(\"latestPrice\").alias(\"high\"),\\\n",
    "            min(\"latestPrice\").alias(\"low\"),\\\n",
    "            min(\"timestamp\").alias(\"open_time\"))\\\n",
    "        .select(\"window.start\",\"window.end\",\"high\",\"low\",\"open_time\")\\\n",
    "        .withWatermark(\"start\", interval)\n",
    "    \n",
    "    # join to get opening price from opening time\n",
    "    chart = interval_values.join(df,interval_values.open_time == df.timestamp, \"left\")\\\n",
    "        .drop(\"open_time\",\"timestamp\")\\\n",
    "        .withColumnRenamed(\"latestPrice\",\"open\")\n",
    "        \n",
    "    return chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(spark, df, update, interval):\n",
    "    # simple moving average for the interval \"interval\"\n",
    "    \n",
    "    windowdf = df.select(window(df.timestamp, interval, update), df.latestPrice)\n",
    "    \n",
    "    windowdf.createOrReplaceTempView(\"windowdf_sql\")\n",
    "    \n",
    "    sma = spark.sql(\"\"\"SELECT windowdf_sql.window AS time, avg(windowdf_sql.latestPrice) AS average\n",
    "                    FROM windowdf_sql\n",
    "                    Group BY windowdf_sql.window\n",
    "                    \"\"\")   \n",
    "    return sma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import clear_output\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stream(df, epoch_id):\n",
    "    df = df.orderBy(\"timestamp\")\n",
    "    df_pd = df.toPandas()\n",
    "    clear_output(wait=True)\n",
    "    df_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average = moving_average(spark, selectDF, \"1 minutes\" ,\"8 minutes\")\n",
    "selectDF\\\n",
    "    .writeStream\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .trigger(processingTime = \"60 seconds\")\\\n",
    "    .foreachBatch(plot_stream)\\\n",
    "    .start()\\\n",
    "    .awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
