{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import datetime\n",
    "import sys\n",
    "from lib import sparkStructuredStreaming\n",
    "import os\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set-up to stream from Kafka topic + read and write from/to Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.5,org.elasticsearch:elasticsearch-spark-20_2.11:7.6.2 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"127.0.0.1:9092\" (local) //\"10.0.0.8:9092\" (BACC)\n",
    "bootstrap = \"127.0.0.1:9092\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "            .builder \\\n",
    "            .appName(\"KafkaIEXStructuredStreaming\") \\\n",
    "            .master(\"local[*]\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "sqlContext = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Historical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read historical data from yahoo finance, write into Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "interval = \"1d\"\n",
    "period = \"5d\"\n",
    "symbol = \"AAPL\"\n",
    "\n",
    "h = sparkStructuredStreaming.history()\n",
    "#h.to_es(symbol,interval,period,sqlContext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Backtesting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Momentum Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol = \"MSFT\"\n",
    "interval = \"1d\"\n",
    "momentum = 20\n",
    "start_capital = 10000\n",
    "commission = 0.002\n",
    "\n",
    "sparkStructuredStreaming.backtest().momentum(symbol, interval, momentum, spark, start_capital, commission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 p/e ratio strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+--------+\n",
      "|               date| Close|position|\n",
      "+-------------------+------+--------+\n",
      "|2019-05-05 22:00:00|205.79|       1|\n",
      "|2019-05-06 22:00:00|200.24|       1|\n",
      "|2019-05-07 22:00:00|200.28|       1|\n",
      "|2019-05-08 22:00:00|198.13|       1|\n",
      "|2019-05-09 22:00:00|195.38|       1|\n",
      "|2019-05-12 22:00:00|184.03|       0|\n",
      "|2019-05-13 22:00:00|186.94|       0|\n",
      "|2019-05-14 22:00:00|189.18|       0|\n",
      "|2019-05-15 22:00:00|188.35|       0|\n",
      "|2019-05-16 22:00:00|187.28|       0|\n",
      "+-------------------+------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from yahoofinancials import YahooFinancials\n",
    "\n",
    "symbol = \"AAPL\"\n",
    "interval = \"1d\"\n",
    "pe_data = h.from_es(symbol,interval,spark)\n",
    "\n",
    "eps = YahooFinancials(symbol).get_earnings_per_share()\n",
    "udf_pe_ratio = udf(lambda x : x/eps)\n",
    "\n",
    "def pe_position_func(pe):\n",
    "    if pe<10:\n",
    "        return -1\n",
    "    elif pe>15:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "udf_pe_position = udf(lambda x : pe_position_func(x))\n",
    "\n",
    "pe_ratio = pe_data.select(\"date\",\"Close\").withColumn(\"pe\",udf_pe_ratio(\"Close\"))\n",
    "pe_position = pe_ratio.withColumn(\"position\",udf_pe_position(\"pe\"))\n",
    "pe_position = pe_position.select(\"date\",\"Close\",\"position\")\n",
    "pe_position.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Trading Simulation / Performance Evaluation with realtime Streams (Spark Streaming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stream real time quotes from Kafka topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use this for elasticsearch, otherwise it won't recognize date field\n",
    "get_datetime_kafka = udf(lambda x : datetime.datetime.fromtimestamp((x-7200000)/ 1000.0).strftime(\"%Y-%m-%d\"'T'\"%H:%M:%S\"))\n",
    "\n",
    "sss = sparkStructuredStreaming.kafka_spark_stream(bootstrap)\n",
    "\n",
    "parsedDF = sss.stream_quotes(spark)       \n",
    "\n",
    "selectDF_es = parsedDF \\\n",
    "        .select(explode(array(\"quote_data\")))\\\n",
    "        .select(\"col.*\",get_datetime_kafka(\"col.latestUpdate\").cast(\"String\").alias(\"date\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize results, either here with Plotly or write results into Elasticsearch -> Kibana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_chart(df,interval):\n",
    "    # use df with \"timestamp\", \"latestPrice\", \"Watermark\"\n",
    "    # get open, high, low prices for each time interval\n",
    "    interval_values = df.groupBy(\n",
    "        window(df.timestamp, interval))\\\n",
    "        .agg(max(\"latestPrice\").alias(\"high\"),\\\n",
    "            min(\"latestPrice\").alias(\"low\"),\\\n",
    "            min(\"timestamp\").alias(\"open_time\"))\\\n",
    "        .select(\"window.start\",\"window.end\",\"high\",\"low\",\"open_time\")\\\n",
    "        .withWatermark(\"start\", interval)\n",
    "    \n",
    "    # join to get opening price from opening time\n",
    "    chart = interval_values.join(df,interval_values.open_time == df.timestamp, \"left\")\\\n",
    "        .drop(\"open_time\",\"timestamp\")\\\n",
    "        .withColumnRenamed(\"latestPrice\",\"open\")\n",
    "        \n",
    "    return chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(spark, df, update, interval):\n",
    "    # simple moving average for the interval \"interval\"\n",
    "    \n",
    "    windowdf = df.select(window(df.timestamp, interval, update), df.latestPrice)\n",
    "    \n",
    "    windowdf.createOrReplaceTempView(\"windowdf_sql\")\n",
    "    \n",
    "    sma = spark.sql(\"\"\"SELECT windowdf_sql.window AS time, avg(windowdf_sql.latestPrice) AS average\n",
    "                    FROM windowdf_sql\n",
    "                    Group BY windowdf_sql.window\n",
    "                    \"\"\")   \n",
    "    return sma"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
