{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import datetime\n",
    "import sys\n",
    "from lib import sparkStructuredStreaming\n",
    "import os\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set-up to stream from Kafka topic + read and write from/to Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.5,org.elasticsearch:elasticsearch-spark-20_2.11:7.6.2 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"127.0.0.1:9092\" (local) //\"10.0.0.8:9092\" (BACC)\n",
    "bootstrap = \"127.0.0.1:9092\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "            .builder \\\n",
    "            .appName(\"KafkaIEXStructuredStreaming\") \\\n",
    "            .master(\"local[*]\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "sqlContext = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Historical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read historical data from yahoo finance, write into HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol=[\"AAPL\",\"MSFT\",\"AMZN\",\"^IXIC\"]\n",
    "period=\"5d\"\n",
    "interval=\"1m\"\n",
    "hdfs_path = \"hdfs://0.0.0.0:19000\"\n",
    "\n",
    "for symbol in symbol:\n",
    "    sparkStructuredStreaming.history().to_hdfs(symbol, interval, period, sqlContext, hdfs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Backtesting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Momentum Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol=[\"AAPL\",\"MSFT\",\"AMZN\"]\n",
    "momentum = 120\n",
    "interval=\"1m\"\n",
    "hdfs_path = \"hdfs://0.0.0.0:19000\"\n",
    "startCap = 10000\n",
    "share = [0.3,0.2,0.3,0.2]\n",
    "commission = 0\n",
    "b = sparkStructuredStreaming.backtest()\n",
    "depotId = 1\n",
    "risk_free = 2\n",
    "\n",
    "position = b.momentum_portfolio_position(symbol, interval, momentum, sqlContext, hdfs_path)\n",
    "depot = b.depot(depotId, symbol, share, position, startCap, commission, risk_free)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### to do: write results into hdfs, try out different strategies/stocks, use best one on realtime data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wert :  10509.339999999995 start-capital :  10000 profit :  509.3399999999947 start-date :  2020-05-04 15:31:00 trades :  2934 performance :  5.093399999999937 %\n",
      "beta :  0.4161927893979215 alpha :  1.1696561072945306\n"
     ]
    }
   ],
   "source": [
    "print(\"wert : \",depot[0],\"start-capital : \",depot[1],\"profit : \",depot[2],\"start-date : \",depot[3],\"trades : \",depot[4],\"performance : \",depot[5],\"%\")\n",
    "print(\"beta : \",depot[6],\"alpha : \",depot[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Trading Simulation / Performance Evaluation with realtime Streams (Spark Streaming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stream real time quotes from Kafka topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use this for elasticsearch, otherwise it won't recognize date field\n",
    "get_datetime_kafka = udf(lambda x : datetime.datetime.fromtimestamp((x-7200000)/ 1000.0).strftime(\"%Y-%m-%d\"'T'\"%H:%M:%S\"))\n",
    "\n",
    "sss = sparkStructuredStreaming.kafka_spark_stream(bootstrap)\n",
    "\n",
    "parsedDF = sss.stream_quotes(spark)       \n",
    "\n",
    "selectDF_es = parsedDF \\\n",
    "        .select(explode(array(\"quote_data\")))\\\n",
    "        .select(\"col.*\",get_datetime_kafka(\"col.latestUpdate\").cast(\"String\").alias(\"date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(spark, df, update, interval):\n",
    "    # simple moving average for the interval \"interval\"\n",
    "    \n",
    "    windowdf = df.select(window(df.timestamp, interval, update), df.latestPrice)\n",
    "    \n",
    "    windowdf.createOrReplaceTempView(\"windowdf_sql\")\n",
    "    \n",
    "    sma = spark.sql(\"\"\"SELECT windowdf_sql.window AS time, avg(windowdf_sql.latestPrice) AS average\n",
    "                    FROM windowdf_sql\n",
    "                    Group BY windowdf_sql.window\n",
    "                    \"\"\")   \n",
    "    return sma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize results, either here or write results into Elasticsearch -> Kibana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
