{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import datetime\n",
    "import sys\n",
    "from lib import sparkStructuredStreaming\n",
    "import os\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set-up to stream from Kafka topic + read and write from/to Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.5,org.elasticsearch:elasticsearch-spark-20_2.11:7.6.2 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"127.0.0.1:9092\" (local) //\"10.0.0.8:9092\" (BACC)\n",
    "bootstrap = \"127.0.0.1:9092\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "            .builder \\\n",
    "            .appName(\"KafkaIEXStructuredStreaming\") \\\n",
    "            .master(\"local[*]\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "sqlContext = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Historical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read historical data from yahoo finance, write into HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol=[\"AAPL\",\"MSFT\",\"AMZN\",\"^IXIC\"]\n",
    "period=\"5d\"\n",
    "interval=\"1m\"\n",
    "hdfs_path = \"hdfs://0.0.0.0:19000\"\n",
    "\n",
    "for symbol in symbol:\n",
    "    sparkStructuredStreaming.history().to_hdfs(symbol, interval, period, sqlContext, hdfs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Backtesting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Momentum Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol=[\"AAPL\",\"MSFT\",\"AMZN\"]\n",
    "#strategy looks at last 120 minutes to calculate average\n",
    "strategy = [\"momentum\",100,110,120,130,140,800]\n",
    "# granularity of historical data\n",
    "interval=\"1m\"\n",
    "hdfs_path = \"hdfs://0.0.0.0:19000\"\n",
    "#start capital\n",
    "startCap = 10000.0\n",
    "# distribution of start-capital between stocks\n",
    "share = [0.3,0.2,0.3,0.2]\n",
    "# regulatory trading fee\n",
    "commission = 0.000119\n",
    "# when testing different strategies each one needs individual id\n",
    "depotId = 8\n",
    "# risk free market return, assumed here 0.1% but is not really clear\n",
    "risk_free = 0.001\n",
    "\n",
    "b = sparkStructuredStreaming.backtest()\n",
    "\n",
    "performance = b.performance(depotId, symbol, share, startCap, commission, risk_free, strategy, interval, hdfs_path, sqlContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------------------+--------------------+-------------+------------------+----------+------+--------------------+------------------+\n",
      "|DepotId|             Value|              Alpha|                Beta|Start-Capital|            Profit|Start-Date|Trades|Performance_Strategy|Performance_Nasdaq|\n",
      "+-------+------------------+-------------------+--------------------+-------------+------------------+----------+------+--------------------+------------------+\n",
      "|      1| 10367.81868400003| 0.8289415795010102|  0.4231281977569605|      10000.0|367.81868400003077|2020-05-04|  2364|   3.678186840000297| 6.711052086563263|\n",
      "|      2| 10309.53392800002|0.17485787444046785| 0.43376936269263766|      10000.0|309.53392800002075|2020-05-04|  2488|   3.095339280000209| 6.711052086563263|\n",
      "|      3|10539.946582000004| 2.4291610041627716| 0.44121192048364566|      10000.0| 539.9465820000041|2020-05-04|  3222|   5.399465820000038| 6.711052086563263|\n",
      "|      4|10529.090374000012| 2.3837693389380634| 0.43177560410089233|      10000.0| 529.0903740000122|2020-05-04|  2854|   5.290903740000119| 6.711052086563263|\n",
      "|      5|10533.506582000029|  2.403865416051798| 0.43537055306731637|      10000.0| 533.5065820000291|2020-05-04|  3222|   5.335065820000295| 6.711052086563263|\n",
      "|      6|      10055.100258| 0.3108750426593393|0.033381503783449985|      10000.0|55.100258000000395|2020-05-04|   418|   0.551002580000004| 6.711052086563263|\n",
      "|      7|      10608.342384|  -1.12081088177219|  1.0783560783277986|      10000.0| 608.3423839999996|2020-05-04|    32|   6.083423840000002| 6.681967274479805|\n",
      "+-------+------------------+-------------------+--------------------+-------------+------------------+----------+------+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_performance = sqlContext.read.format('parquet').load(hdfs_path+\"/performance\")\n",
    "df_performance.show(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+------------+------------------+--------------------+\n",
      "|DepotId|Start-Caputal|    Strategy|              ISIN|               Share|\n",
      "+-------+-------------+------------+------------------+--------------------+\n",
      "|      1|      10000.0| momentum100|[AAPL, MSFT, AMZN]|[0.3, 0.2, 0.3, 0.2]|\n",
      "|      2|      10000.0| momentum110|[AAPL, MSFT, AMZN]|[0.3, 0.2, 0.3, 0.2]|\n",
      "|      3|      10000.0| momentum120|[AAPL, MSFT, AMZN]|[0.3, 0.2, 0.3, 0.2]|\n",
      "|      4|      10000.0| momentum130|[AAPL, MSFT, AMZN]|[0.3, 0.2, 0.3, 0.2]|\n",
      "|      5|      10000.0| momentum140|[AAPL, MSFT, AMZN]|[0.3, 0.2, 0.3, 0.2]|\n",
      "|      6|      10000.0| momentum800|[AAPL, MSFT, AMZN]|[0.3, 0.2, 0.3, 0.2]|\n",
      "|      7|      10000.0|Buy and Hold|[AAPL, MSFT, AMZN]|[0.3, 0.2, 0.3, 0.2]|\n",
      "+-------+-------------+------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_depot = sqlContext.read.format('parquet').load(hdfs_path+\"/depot\")\n",
    "df_depot.orderBy(\"DepotId\").show(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Trading Simulation / Performance Evaluation with realtime Streams (Spark Streaming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stream real time quotes from Kafka topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_datetime = udf(lambda x : datetime.datetime.fromtimestamp(x/ 1000.0).strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "sss = sparkStructuredStreaming.kafka_spark_stream(bootstrap)\n",
    "\n",
    "parsedDF = sss.stream_quotes(spark)       \n",
    "\n",
    "selectDF = parsedDF \\\n",
    "        .select(explode(array(\"quote_data\")))\\\n",
    "        .select(\"col.*\",get_datetime(\"col.latestUpdate\").cast(\"Timestamp\").alias(\"Datetime\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "startCap = 10000.0\n",
    "symbol = [\"AAPL\"]\n",
    "share = [1.0]\n",
    "\n",
    "selectDF = selectDF.select(\"Datetime\",\"symbol\",\"latestPrice\").withColumn(\"startCap\",lit(startCap))\n",
    "sss.write_console(selectDF).awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(spark, df, update, interval):\n",
    "    # simple moving average for the interval \"interval\"\n",
    "    \n",
    "    windowdf = df.select(window(df.timestamp, interval, update), df.latestPrice)\n",
    "    \n",
    "    windowdf.createOrReplaceTempView(\"windowdf_sql\")\n",
    "    \n",
    "    sma = spark.sql(\"\"\"SELECT windowdf_sql.window AS time, avg(windowdf_sql.latestPrice) AS average\n",
    "                    FROM windowdf_sql\n",
    "                    Group BY windowdf_sql.window\n",
    "                    \"\"\")   \n",
    "    return sma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize results, either here or write results into Elasticsearch -> Kibana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
