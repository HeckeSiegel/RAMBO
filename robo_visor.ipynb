{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import datetime\n",
    "import sys\n",
    "from lib import sparkStructuredStreaming\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set-up to stream from Kafka topic + read and write from/to Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.5,org.elasticsearch:elasticsearch-spark-20_2.11:7.6.2 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"127.0.0.1:9092\" (local) //\"10.0.0.8:9092\" (BACC)\n",
    "bootstrap = \"127.0.0.1:9092\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "            .builder \\\n",
    "            .appName(\"KafkaIEXStructuredStreaming\") \\\n",
    "            .master(\"local[*]\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "sqlContext = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Implement Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_strategy(close):\n",
    "    if pe_ratio < 10:\n",
    "        action = \"buy\"\n",
    "    elif pe_ratio > 15:\n",
    "        action = \"sell\"\n",
    "    else:\n",
    "        action = \"hold\"\n",
    "        \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Backtesting with static Dataframes with Historical Data from Yahoo Finance (Pyspark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Read historical data from yahoo finance, write into Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import history\n",
    "interval = \"60m\"\n",
    "period = \"5d\"\n",
    "symbol = \"AAPL\"\n",
    "\n",
    "h = history.history()\n",
    "h.to_es(symbol,interval,period,sqlContext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Read historical data from Elasticsearch into Spark Dataframe and evaluate Strategy with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+------+-------+-------------------+------+\n",
      "| Close|  High|   Low|  Open| Volume|               date|symbol|\n",
      "+------+------+------+------+-------+-------------------+------+\n",
      "|288.33|288.55|283.96|284.73|9996261|2020-04-29 13:30:00|  AAPL|\n",
      "|287.53|289.67|287.25|288.33|4689060|2020-04-29 14:30:00|  AAPL|\n",
      "|287.11|287.88|286.73|287.56|2832405|2020-04-29 15:30:00|  AAPL|\n",
      "|287.16|288.02|286.99|287.18|2503534|2020-04-29 16:30:00|  AAPL|\n",
      "|287.36| 287.7|286.33|287.17|2768467|2020-04-29 17:30:00|  AAPL|\n",
      "| 288.7|288.85|287.01|287.39|3338954|2020-04-29 18:30:00|  AAPL|\n",
      "|287.83|289.33|287.33| 288.7|3340075|2020-04-29 19:30:00|  AAPL|\n",
      "| 292.2|293.32|288.46|289.96|9652057|2020-04-30 13:30:00|  AAPL|\n",
      "|291.06|292.45|290.35|292.21|3630126|2020-04-30 14:30:00|  AAPL|\n",
      "|289.49| 291.4|288.88|291.03|4048306|2020-04-30 15:30:00|  AAPL|\n",
      "+------+------+------+------+-------+-------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = h.from_es(symbol,interval,spark)\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.2880003567661875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'udf_pe_ratio = udf(lambda x : x/eps)\\ndf = df.select(\"date\",\"Close\",udf_pe_ratio(\"Close\").alias(\"pe_ratio\"))\\ndf.orderBy(\"pe_ratio\").show(20)'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from yahoofinancials import YahooFinancials\n",
    "\n",
    "#earnings per share to calculate p/e ratio\n",
    "symbol = \"FB\"\n",
    "eps = YahooFinancials(symbol).get_earnings_per_share()\n",
    "print(eps)\n",
    "'''udf_pe_ratio = udf(lambda x : x/eps)\n",
    "df = df.select(\"date\",\"Close\",udf_pe_ratio(\"Close\").alias(\"pe_ratio\"))\n",
    "df.orderBy(\"pe_ratio\").show(20)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Trading Simulation / Performance Evaluation with realtime Streams (Spark Streaming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stream real time quotes from Kafka topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use this for elasticsearch, otherwise it won't recognize date field\n",
    "get_datetime_kafka = udf(lambda x : datetime.datetime.fromtimestamp((x-7200000)/ 1000.0).strftime(\"%Y-%m-%d\"'T'\"%H:%M:%S\"))\n",
    "\n",
    "sss = sparkStructuredStreaming.kafka_spark_stream(bootstrap)\n",
    "\n",
    "parsedDF = sss.stream_quotes(spark)       \n",
    "\n",
    "selectDF_es = parsedDF \\\n",
    "        .select(explode(array(\"quote_data\")))\\\n",
    "        .select(\"col.*\",get_datetime_kafka(\"col.latestUpdate\").cast(\"String\").alias(\"date\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize results, either here with Plotly or write results into Elasticsearch -> Kibana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_chart(df,interval):\n",
    "    # use df with \"timestamp\", \"latestPrice\", \"Watermark\"\n",
    "    # get open, high, low prices for each time interval\n",
    "    interval_values = df.groupBy(\n",
    "        window(df.timestamp, interval))\\\n",
    "        .agg(max(\"latestPrice\").alias(\"high\"),\\\n",
    "            min(\"latestPrice\").alias(\"low\"),\\\n",
    "            min(\"timestamp\").alias(\"open_time\"))\\\n",
    "        .select(\"window.start\",\"window.end\",\"high\",\"low\",\"open_time\")\\\n",
    "        .withWatermark(\"start\", interval)\n",
    "    \n",
    "    # join to get opening price from opening time\n",
    "    chart = interval_values.join(df,interval_values.open_time == df.timestamp, \"left\")\\\n",
    "        .drop(\"open_time\",\"timestamp\")\\\n",
    "        .withColumnRenamed(\"latestPrice\",\"open\")\n",
    "        \n",
    "    return chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(spark, df, update, interval):\n",
    "    # simple moving average for the interval \"interval\"\n",
    "    \n",
    "    windowdf = df.select(window(df.timestamp, interval, update), df.latestPrice)\n",
    "    \n",
    "    windowdf.createOrReplaceTempView(\"windowdf_sql\")\n",
    "    \n",
    "    sma = spark.sql(\"\"\"SELECT windowdf_sql.window AS time, avg(windowdf_sql.latestPrice) AS average\n",
    "                    FROM windowdf_sql\n",
    "                    Group BY windowdf_sql.window\n",
    "                    \"\"\")   \n",
    "    return sma"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
